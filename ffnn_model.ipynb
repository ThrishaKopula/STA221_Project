{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea93ff9-2037-44b3-aa6e-64099567c426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV created successfully as 'ffnn.csv'.\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the first CSV\n",
    "# df_comments = pd.read_parquet('data/comments.parquet.brotli', engine='pyarrow')\n",
    "\n",
    "# # Load the second CSV\n",
    "# df_roberta = pd.read_csv('data/comment_sentiment.csv')\n",
    "\n",
    "# # Merge the two dataframes on the 'comment_id' column\n",
    "# merged_df = pd.merge(df_comments, df_roberta, on='comment_id', how='inner')\n",
    "\n",
    "# # Save the merged dataframe to a new CSV\n",
    "# merged_df.to_csv('data/ffnn.csv', index=False)\n",
    "\n",
    "# print(\"Merged CSV created successfully as 'ffnn.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ec1e6-adbc-4a01-b13a-8dc7e75fb755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8a3a2cd-d0d4-4dab-8d18-9ff185b4c3e4",
   "metadata": {},
   "source": [
    "### **FFNN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b29a8655-6785-44cf-ad6b-6f793042e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d3b0cc1d-12b3-4666-86a8-f60ef64cec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv('data/comment_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "70647548-f701-4df4-846e-8c234fc2c261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming the CSV has columns: 'comment' (text) and 'sentiment' (-1, 0, 1)\n",
    "comments = data['comment_content']\n",
    "sentiments = data['RoBERTa_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "75352222-7044-4cd1-86f5-41b30fae6fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical format\n",
    "y = to_categorical(sentiments, num_classes=3)  # Convert to one-hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f3baa43a-8621-4792-bc4d-927d9b4381e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa_sentiment\n",
      "-1    457913\n",
      " 0    410038\n",
      " 1    170166\n",
      "Name: count, dtype: int64\n",
      "Unique values in sentiments: [ 0 -1  1]\n",
      "Unique values in RoBERTa_sentiment: [ 0 -1  1]\n"
     ]
    }
   ],
   "source": [
    "print(sentiments.value_counts())\n",
    "\n",
    "print(\"Unique values in sentiments:\", sentiments.unique())\n",
    "print(\"Unique values in RoBERTa_sentiment:\", data['RoBERTa_sentiment'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bab4df29-5743-46d8-8bc6-d5458dbf4910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2799c7e0-c021-4b31-9f1d-cf41268427f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build the FFNN Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128),  # Embedding layer\n",
    "    Flatten(),  # Flatten the output from the embedding layer\n",
    "    Dense(128, activation='relu'),  # Hidden layer 1\n",
    "    Dense(64, activation='relu'),  # Hidden layer 2\n",
    "    Dense(3, activation='softmax')  # Output layer for multi-class classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e5ab2c4b-8a1c-4615-80be-966278adeaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compile the Model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e5c17a7e-b070-4e14-8bfa-0976fc398509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20763/20763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6ms/step - accuracy: 0.7005 - loss: 0.6802 - val_accuracy: 0.7523 - val_loss: 0.5842\n",
      "Epoch 2/10\n",
      "\u001b[1m20763/20763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6ms/step - accuracy: 0.7894 - loss: 0.5063 - val_accuracy: 0.7564 - val_loss: 0.5846\n",
      "Epoch 3/10\n",
      "\u001b[1m20763/20763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 6ms/step - accuracy: 0.8520 - loss: 0.3708 - val_accuracy: 0.7528 - val_loss: 0.6536\n",
      "Epoch 4/10\n",
      "\u001b[1m20763/20763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6ms/step - accuracy: 0.8937 - loss: 0.2740 - val_accuracy: 0.7457 - val_loss: 0.7730\n",
      "Epoch 5/10\n",
      "\u001b[1m20763/20763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6ms/step - accuracy: 0.9187 - loss: 0.2123 - val_accuracy: 0.7409 - val_loss: 0.9738\n",
      "Epoch 6/10\n",
      "\u001b[1m20763/20763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6ms/step - accuracy: 0.9358 - loss: 0.1707 - val_accuracy: 0.7410 - val_loss: 1.0824\n",
      "Epoch 7/10\n",
      "\u001b[1m20763/20763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6ms/step - accuracy: 0.9470 - loss: 0.1427 - val_accuracy: 0.7355 - val_loss: 1.2128\n",
      "Epoch 8/10\n",
      "\u001b[1m20763/20763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6ms/step - accuracy: 0.9562 - loss: 0.1213 - val_accuracy: 0.7332 - val_loss: 1.3674\n",
      "Epoch 9/10\n",
      "\u001b[1m20763/20763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6ms/step - accuracy: 0.9615 - loss: 0.1064 - val_accuracy: 0.7330 - val_loss: 1.5435\n",
      "Epoch 10/10\n",
      "\u001b[1m20763/20763\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6ms/step - accuracy: 0.9660 - loss: 0.0944 - val_accuracy: 0.7337 - val_loss: 1.6459\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "436f1e69-537f-43b3-b7e6-76648de657dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6489/6489\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 797us/step\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Evaluate the Model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels\n",
    "y_test_classes = np.argmax(y_test, axis=1)  # Convert one-hot back to class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a3ad889d-1565-4557-9d40-e46948297ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.70      0.73      0.71     81713\n",
      "     Neutral       0.70      0.64      0.67     33906\n",
      "    Positive       0.77      0.77      0.77     92005\n",
      "\n",
      "    accuracy                           0.73    207624\n",
      "   macro avg       0.72      0.71      0.72    207624\n",
      "weighted avg       0.73      0.73      0.73    207624\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "print(classification_report(y_test_classes, y_pred_classes, target_names=['Negative', 'Neutral', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c3dd1-95a1-45a7-bf93-ef037a873aae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
